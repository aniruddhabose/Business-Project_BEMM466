{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21f519f-7335-4fc4-8a2c-b2f7b08eadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\n",
      "Raw: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\raw\n",
      "Interim: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\interim\n",
      "Processed: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\processed\n",
      "No files found in /data/raw yet. Place your originals there and re-run.\n",
      "Audit summary written -> C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\processed\\audit_summary.csv\n",
      "No tidy data produced; master panel not created.\n"
     ]
    }
   ],
   "source": [
    "# Phase A — Data audit & single “master” panel\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0) Setup & paths\n",
    "# -------------------------------------------------------------------\n",
    "# Optional: quiet noisy pandas parsing warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format\")\n",
    "\n",
    "PROJ_ROOT = Path.cwd().resolve()  # run the notebook from the project root\n",
    "DATA_RAW = PROJ_ROOT / \"data\" / \"raw\"\n",
    "DATA_INTERIM = PROJ_ROOT / \"data\" / \"interim\"\n",
    "DATA_PROCESSED = PROJ_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "for p in [DATA_RAW, DATA_INTERIM, DATA_PROCESSED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJ_ROOT)\n",
    "print(\"Raw:\", DATA_RAW)\n",
    "print(\"Interim:\", DATA_INTERIM)\n",
    "print(\"Processed:\", DATA_PROCESSED)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Helpers: column cleanup, date detection, value detection\n",
    "# -------------------------------------------------------------------\n",
    "def _clean_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w_]+\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def _parse_year_quarter_like(s):\n",
    "    \"\"\"\n",
    "    Try parsing common Y-Q formats:\n",
    "      '2020 Q1', '2020Q1', '2020 Quarter 1', 'Q1 2020'\n",
    "    Returns pandas datetime (end of quarter).\n",
    "    \"\"\"\n",
    "    txt = s.astype(str).str.strip()\n",
    "\n",
    "    # Normalize various quarter tokens to a uniform form\n",
    "    txt = (\n",
    "        txt.str.replace(r\"quarter\\s*\", \"Q\", flags=re.I, regex=True)\n",
    "           .str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Convert 'Q12020' -> '2020Q1'\n",
    "    txt = txt.str.replace(r\"^Q([1-4])(\\d{4})$\", r\"\\2Q\\1\", regex=True)\n",
    "\n",
    "    # Keep those that look like 'YYYYQq'\n",
    "    mask = txt.str.match(r\"^\\d{4}Q[1-4]$\", na=False)\n",
    "    yq = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
    "    if mask.any():\n",
    "        per = pd.PeriodIndex(txt[mask], freq=\"Q\")\n",
    "        yq.loc[mask] = per.to_timestamp(how=\"end\")\n",
    "    return yq\n",
    "\n",
    "def _try_parse_year_quarter_cols(df):\n",
    "    \"\"\"\n",
    "    If there are separate 'year' and 'quarter' columns, build a datetime series\n",
    "    at quarter-end.\n",
    "    \"\"\"\n",
    "    year_cols = [c for c in df.columns if c in {\"year\", \"yr\"}]\n",
    "    q_cols = [c for c in df.columns if c in {\"quarter\", \"qtr\", \"q\"}]\n",
    "    if year_cols and q_cols:\n",
    "        yc, qc = year_cols[0], q_cols[0]\n",
    "        y = pd.to_numeric(df[yc], errors=\"coerce\")\n",
    "        q = pd.to_numeric(df[qc], errors=\"coerce\")\n",
    "        ok = y.notna() & q.notna() & q.between(1, 4)\n",
    "        per = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "        if ok.any():\n",
    "            idx = ok[ok].index\n",
    "            per.loc[idx] = pd.PeriodIndex(\n",
    "                (y.loc[idx].astype(int).astype(str) + \"Q\" + q.loc[idx].astype(int).astype(str)).values,\n",
    "                freq=\"Q\"\n",
    "            ).to_timestamp(how=\"end\")\n",
    "        return per\n",
    "    return pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "def _first_datetime_col(df):\n",
    "    \"\"\"\n",
    "    Find/construct a usable datetime column (monthly/quarterly/annual).\n",
    "    Returns a datetime Series or None.\n",
    "    \"\"\"\n",
    "    # Try explicit date-like columns first\n",
    "    candidates = [\"period\", \"date\", \"time\", \"month\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "            if dt.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "                return dt\n",
    "\n",
    "    # Try a single 'year' column (assume Dec-31)\n",
    "    if \"year\" in df.columns:\n",
    "        y = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "        if y.notna().any():\n",
    "            return pd.to_datetime(y.astype(\"Int64\").astype(str) + \"-12-31\", errors=\"coerce\")\n",
    "\n",
    "    # Try year+quarter columns\n",
    "    yq_cols = _try_parse_year_quarter_cols(df)\n",
    "    if yq_cols.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "        return yq_cols\n",
    "\n",
    "    # Try generic text quarter formats in any object col\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            yq_free = _parse_year_quarter_like(df[c])\n",
    "            if yq_free.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "                return yq_free\n",
    "\n",
    "    # Brute force: try every column via to_datetime\n",
    "    for c in df.columns:\n",
    "        dt = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "        if dt.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "            return dt\n",
    "    return None\n",
    "\n",
    "def _numeric_candidate_columns(df):\n",
    "    exclude = {\"period\", \"date\", \"time\", \"month\", \"quarter\", \"qtr\", \"year\", \"yr\", \"q\"}\n",
    "    num_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "            num_cols.append(c)\n",
    "    return num_cols\n",
    "\n",
    "def _guess_freq(period_series):\n",
    "    \"\"\"\n",
    "    Roughly guess frequency from median day gaps.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(period_series.dropna(), errors=\"coerce\").sort_values().unique()\n",
    "    if len(s) < 3:\n",
    "        return \"unknown\"\n",
    "\n",
    "    s = pd.Series(s).astype(\"datetime64[ns]\")\n",
    "    deltas = s.diff().iloc[1:]  # TimedeltaIndex/Series\n",
    "    days = deltas / np.timedelta64(1, \"D\")  # float days\n",
    "    med = float(np.nanmedian(days))\n",
    "\n",
    "    if 27 <= med <= 35:\n",
    "        return \"monthly\"\n",
    "    if 80 <= med <= 100:\n",
    "        return \"quarterly\"\n",
    "    if 360 <= med <= 370:\n",
    "        return \"annual\"\n",
    "    return \"irregular\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Read any file (csv/xlsx/xls) and tidy to long format\n",
    "# -------------------------------------------------------------------\n",
    "def read_any(path):\n",
    "    suf = path.suffix.lower()\n",
    "    try:\n",
    "        if suf == \".csv\":\n",
    "            try:\n",
    "                return pd.read_csv(path)\n",
    "            except UnicodeDecodeError:\n",
    "                return pd.read_csv(path, encoding=\"latin-1\")\n",
    "        elif suf in {\".xlsx\", \".xls\"}:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                # sheet_name=0 by default; override later via per-file logic if needed\n",
    "                return pd.read_excel(path, sheet_name=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {suf}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read {path.name}: {e}\") from e\n",
    "\n",
    "def tidy_any(path):\n",
    "    raw = read_any(path)\n",
    "    raw = _clean_cols(raw)\n",
    "\n",
    "    # Drop fully empty columns/rows\n",
    "    raw = raw.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\")\n",
    "    if raw.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Detect period column (datetime-like)\n",
    "    period = _first_datetime_col(raw)\n",
    "    if period is None:\n",
    "        # As a last resort: look for a column called 'year' even if sparse\n",
    "        if \"year\" in raw.columns:\n",
    "            y = pd.to_numeric(raw[\"year\"], errors=\"coerce\")\n",
    "            period = pd.to_datetime(y.astype(\"Int64\").astype(str) + \"-12-31\", errors=\"coerce\")\n",
    "        else:\n",
    "            # cannot structure as time series; return empty tidy frame\n",
    "            return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Identify numeric measure columns\n",
    "    num_cols = _numeric_candidate_columns(raw)\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Slice to period + numeric cols\n",
    "    df = pd.DataFrame({\"period\": pd.to_datetime(period, errors=\"coerce\")})\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(raw[c], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"period\"])\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Long format\n",
    "    tidy = df.melt(id_vars=\"period\", value_vars=num_cols, var_name=\"measure\", value_name=\"value\")\n",
    "    tidy[\"source_file\"] = path.name\n",
    "    tidy[\"measure\"] = tidy[\"measure\"].str.replace(r\"__+\", \"_\", regex=True).str.strip(\"_\")\n",
    "    tidy = tidy.dropna(subset=[\"value\"]).sort_values([\"measure\", \"period\"]).reset_index(drop=True)\n",
    "\n",
    "    return tidy[[\"source_file\",\"measure\",\"period\",\"value\"]]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Process all raw files -> interim tidy CSVs + audit\n",
    "# -------------------------------------------------------------------\n",
    "patterns = [\"*.csv\", \"*.xlsx\", \"*.xls\"]\n",
    "raw_files = []\n",
    "for pat in patterns:\n",
    "    raw_files.extend(DATA_RAW.rglob(pat))\n",
    "\n",
    "if not raw_files:\n",
    "    print(\"No files found in /data/raw yet. Place your originals there and re-run.\")\n",
    "\n",
    "all_tidy = []\n",
    "audit_rows = []\n",
    "\n",
    "for f in sorted(raw_files):\n",
    "    tidy_df = tidy_any(f)\n",
    "    out_path = DATA_INTERIM / f\"{f.stem}_tidy.csv\"\n",
    "    tidy_df.to_csv(out_path, index=False)\n",
    "\n",
    "    # compute audit stats\n",
    "    if tidy_df.empty:\n",
    "        audit_rows.append({\n",
    "            \"source_file\": f.name,\n",
    "            \"tidy_rows\": 0,\n",
    "            \"measures\": 0,\n",
    "            \"period_min\": pd.NaT,\n",
    "            \"period_max\": pd.NaT,\n",
    "            \"freq_guess\": \"unknown\",\n",
    "            \"missing_values\": 0\n",
    "        })\n",
    "    else:\n",
    "        freq = _guess_freq(tidy_df[\"period\"])\n",
    "        audit_rows.append({\n",
    "            \"source_file\": f.name,\n",
    "            \"tidy_rows\": len(tidy_df),\n",
    "            \"measures\": tidy_df[\"measure\"].nunique(),\n",
    "            \"period_min\": tidy_df[\"period\"].min(),\n",
    "            \"period_max\": tidy_df[\"period\"].max(),\n",
    "            \"freq_guess\": freq,\n",
    "            \"missing_values\": tidy_df[\"value\"].isna().sum()\n",
    "        })\n",
    "        all_tidy.append(tidy_df)\n",
    "\n",
    "audit = pd.DataFrame(audit_rows)\n",
    "audit_path = DATA_PROCESSED / \"audit_summary.csv\"\n",
    "audit.to_csv(audit_path, index=False)\n",
    "print(f\"Audit summary written -> {audit_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Build master panel (concat all tidy files)\n",
    "# -------------------------------------------------------------------\n",
    "if all_tidy:\n",
    "    master = pd.concat(all_tidy, ignore_index=True).drop_duplicates()\n",
    "    master[\"period\"] = pd.to_datetime(master[\"period\"], errors=\"coerce\")\n",
    "    master = master.dropna(subset=[\"period\"]).sort_values([\"measure\",\"period\"]).reset_index(drop=True)\n",
    "\n",
    "    master_path = DATA_PROCESSED / \"master_panel.csv\"\n",
    "    master.to_csv(master_path, index=False)\n",
    "    print(f\"Master panel written -> {master_path} ({len(master):,} rows)\")\n",
    "else:\n",
    "    print(\"No tidy data produced; master panel not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55922ca-4b94-4d15-afe8-9fe8ecd98e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90d3306-e94c-4902-ab72-9f294e4d8d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\n",
      "Raw: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\raw\n",
      "Interim: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\interim\n",
      "Processed: C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\processed\n",
      "Audit summary written -> C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\processed\\audit_summary.csv\n",
      "Master panel written -> C:\\Users\\Aniruddha\\Desktop\\Business Project_BEMM466\\Project Code\\data\\processed\\master_panel.csv (1,774 rows)\n"
     ]
    }
   ],
   "source": [
    "# Phase A — Data audit & single “master” panel\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0) Setup & paths\n",
    "# -------------------------------------------------------------------\n",
    "# Optional: quiet noisy pandas parsing warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format\")\n",
    "\n",
    "PROJ_ROOT = Path.cwd().resolve()  # run the notebook from the project root\n",
    "DATA_RAW = PROJ_ROOT / \"data\" / \"raw\"\n",
    "DATA_INTERIM = PROJ_ROOT / \"data\" / \"interim\"\n",
    "DATA_PROCESSED = PROJ_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "for p in [DATA_RAW, DATA_INTERIM, DATA_PROCESSED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJ_ROOT)\n",
    "print(\"Raw:\", DATA_RAW)\n",
    "print(\"Interim:\", DATA_INTERIM)\n",
    "print(\"Processed:\", DATA_PROCESSED)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Helpers: column cleanup, date detection, value detection\n",
    "# -------------------------------------------------------------------\n",
    "def _clean_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w_]+\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def _parse_year_quarter_like(s):\n",
    "    \"\"\"\n",
    "    Try parsing common Y-Q formats:\n",
    "      '2020 Q1', '2020Q1', '2020 Quarter 1', 'Q1 2020'\n",
    "    Returns pandas datetime (end of quarter).\n",
    "    \"\"\"\n",
    "    txt = s.astype(str).str.strip()\n",
    "\n",
    "    # Normalize various quarter tokens to a uniform form\n",
    "    txt = (\n",
    "        txt.str.replace(r\"quarter\\s*\", \"Q\", flags=re.I, regex=True)\n",
    "           .str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Convert 'Q12020' -> '2020Q1'\n",
    "    txt = txt.str.replace(r\"^Q([1-4])(\\d{4})$\", r\"\\2Q\\1\", regex=True)\n",
    "\n",
    "    # Keep those that look like 'YYYYQq'\n",
    "    mask = txt.str.match(r\"^\\d{4}Q[1-4]$\", na=False)\n",
    "    yq = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
    "    if mask.any():\n",
    "        per = pd.PeriodIndex(txt[mask], freq=\"Q\")\n",
    "        yq.loc[mask] = per.to_timestamp(how=\"end\")\n",
    "    return yq\n",
    "\n",
    "def _try_parse_year_quarter_cols(df):\n",
    "    \"\"\"\n",
    "    If there are separate 'year' and 'quarter' columns, build a datetime series\n",
    "    at quarter-end.\n",
    "    \"\"\"\n",
    "    year_cols = [c for c in df.columns if c in {\"year\", \"yr\"}]\n",
    "    q_cols = [c for c in df.columns if c in {\"quarter\", \"qtr\", \"q\"}]\n",
    "    if year_cols and q_cols:\n",
    "        yc, qc = year_cols[0], q_cols[0]\n",
    "        y = pd.to_numeric(df[yc], errors=\"coerce\")\n",
    "        q = pd.to_numeric(df[qc], errors=\"coerce\")\n",
    "        ok = y.notna() & q.notna() & q.between(1, 4)\n",
    "        per = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "        if ok.any():\n",
    "            idx = ok[ok].index\n",
    "            per.loc[idx] = pd.PeriodIndex(\n",
    "                (y.loc[idx].astype(int).astype(str) + \"Q\" + q.loc[idx].astype(int).astype(str)).values,\n",
    "                freq=\"Q\"\n",
    "            ).to_timestamp(how=\"end\")\n",
    "        return per\n",
    "    return pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "def _first_datetime_col(df):\n",
    "    \"\"\"\n",
    "    Find/construct a usable datetime column (monthly/quarterly/annual).\n",
    "    Returns a datetime Series or None.\n",
    "    \"\"\"\n",
    "    # Try explicit date-like columns first\n",
    "    candidates = [\"period\", \"date\", \"time\", \"month\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "            if dt.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "                return dt\n",
    "\n",
    "    # Try a single 'year' column (assume Dec-31)\n",
    "    if \"year\" in df.columns:\n",
    "        y = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "        if y.notna().any():\n",
    "            return pd.to_datetime(y.astype(\"Int64\").astype(str) + \"-12-31\", errors=\"coerce\")\n",
    "\n",
    "    # Try year+quarter columns\n",
    "    yq_cols = _try_parse_year_quarter_cols(df)\n",
    "    if yq_cols.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "        return yq_cols\n",
    "\n",
    "    # Try generic text quarter formats in any object col\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            yq_free = _parse_year_quarter_like(df[c])\n",
    "            if yq_free.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "                return yq_free\n",
    "\n",
    "    # Brute force: try every column via to_datetime\n",
    "    for c in df.columns:\n",
    "        dt = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "        if dt.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "            return dt\n",
    "    return None\n",
    "\n",
    "def _numeric_candidate_columns(df):\n",
    "    exclude = {\"period\", \"date\", \"time\", \"month\", \"quarter\", \"qtr\", \"year\", \"yr\", \"q\"}\n",
    "    num_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().sum() >= max(3, int(0.4 * len(df))):\n",
    "            num_cols.append(c)\n",
    "    return num_cols\n",
    "\n",
    "def _guess_freq(period_series):\n",
    "    \"\"\"\n",
    "    Roughly guess frequency from median day gaps.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(period_series.dropna(), errors=\"coerce\").sort_values().unique()\n",
    "    if len(s) < 3:\n",
    "        return \"unknown\"\n",
    "\n",
    "    s = pd.Series(s).astype(\"datetime64[ns]\")\n",
    "    deltas = s.diff().iloc[1:]  # TimedeltaIndex/Series\n",
    "    days = deltas / np.timedelta64(1, \"D\")  # float days\n",
    "    med = float(np.nanmedian(days))\n",
    "\n",
    "    if 27 <= med <= 35:\n",
    "        return \"monthly\"\n",
    "    if 80 <= med <= 100:\n",
    "        return \"quarterly\"\n",
    "    if 360 <= med <= 370:\n",
    "        return \"annual\"\n",
    "    return \"irregular\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Read any file (csv/xlsx/xls) and tidy to long format\n",
    "# -------------------------------------------------------------------\n",
    "def read_any(path):\n",
    "    suf = path.suffix.lower()\n",
    "    try:\n",
    "        if suf == \".csv\":\n",
    "            try:\n",
    "                return pd.read_csv(path)\n",
    "            except UnicodeDecodeError:\n",
    "                return pd.read_csv(path, encoding=\"latin-1\")\n",
    "        elif suf in {\".xlsx\", \".xls\"}:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                # sheet_name=0 by default; override later via per-file logic if needed\n",
    "                return pd.read_excel(path, sheet_name=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {suf}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read {path.name}: {e}\") from e\n",
    "\n",
    "def tidy_any(path):\n",
    "    raw = read_any(path)\n",
    "    raw = _clean_cols(raw)\n",
    "\n",
    "    # Drop fully empty columns/rows\n",
    "    raw = raw.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\")\n",
    "    if raw.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Detect period column (datetime-like)\n",
    "    period = _first_datetime_col(raw)\n",
    "    if period is None:\n",
    "        # As a last resort: look for a column called 'year' even if sparse\n",
    "        if \"year\" in raw.columns:\n",
    "            y = pd.to_numeric(raw[\"year\"], errors=\"coerce\")\n",
    "            period = pd.to_datetime(y.astype(\"Int64\").astype(str) + \"-12-31\", errors=\"coerce\")\n",
    "        else:\n",
    "            # cannot structure as time series; return empty tidy frame\n",
    "            return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Identify numeric measure columns\n",
    "    num_cols = _numeric_candidate_columns(raw)\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Slice to period + numeric cols\n",
    "    df = pd.DataFrame({\"period\": pd.to_datetime(period, errors=\"coerce\")})\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(raw[c], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"period\"])\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\",\"measure\",\"period\",\"value\"])\n",
    "\n",
    "    # Long format\n",
    "    tidy = df.melt(id_vars=\"period\", value_vars=num_cols, var_name=\"measure\", value_name=\"value\")\n",
    "    tidy[\"source_file\"] = path.name\n",
    "    tidy[\"measure\"] = tidy[\"measure\"].str.replace(r\"__+\", \"_\", regex=True).str.strip(\"_\")\n",
    "    tidy = tidy.dropna(subset=[\"value\"]).sort_values([\"measure\", \"period\"]).reset_index(drop=True)\n",
    "\n",
    "    return tidy[[\"source_file\",\"measure\",\"period\",\"value\"]]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Process all raw files -> interim tidy CSVs + audit\n",
    "# -------------------------------------------------------------------\n",
    "patterns = [\"*.csv\", \"*.xlsx\", \"*.xls\"]\n",
    "raw_files = []\n",
    "for pat in patterns:\n",
    "    raw_files.extend(DATA_RAW.rglob(pat))\n",
    "\n",
    "if not raw_files:\n",
    "    print(\"No files found in /data/raw yet. Place your originals there and re-run.\")\n",
    "\n",
    "all_tidy = []\n",
    "audit_rows = []\n",
    "\n",
    "for f in sorted(raw_files):\n",
    "    tidy_df = tidy_any(f)\n",
    "    out_path = DATA_INTERIM / f\"{f.stem}_tidy.csv\"\n",
    "    tidy_df.to_csv(out_path, index=False)\n",
    "\n",
    "    # compute audit stats\n",
    "    if tidy_df.empty:\n",
    "        audit_rows.append({\n",
    "            \"source_file\": f.name,\n",
    "            \"tidy_rows\": 0,\n",
    "            \"measures\": 0,\n",
    "            \"period_min\": pd.NaT,\n",
    "            \"period_max\": pd.NaT,\n",
    "            \"freq_guess\": \"unknown\",\n",
    "            \"missing_values\": 0\n",
    "        })\n",
    "    else:\n",
    "        freq = _guess_freq(tidy_df[\"period\"])\n",
    "        audit_rows.append({\n",
    "            \"source_file\": f.name,\n",
    "            \"tidy_rows\": len(tidy_df),\n",
    "            \"measures\": tidy_df[\"measure\"].nunique(),\n",
    "            \"period_min\": tidy_df[\"period\"].min(),\n",
    "            \"period_max\": tidy_df[\"period\"].max(),\n",
    "            \"freq_guess\": freq,\n",
    "            \"missing_values\": tidy_df[\"value\"].isna().sum()\n",
    "        })\n",
    "        all_tidy.append(tidy_df)\n",
    "\n",
    "audit = pd.DataFrame(audit_rows)\n",
    "audit_path = DATA_PROCESSED / \"audit_summary.csv\"\n",
    "audit.to_csv(audit_path, index=False)\n",
    "print(f\"Audit summary written -> {audit_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Build master panel (concat all tidy files)\n",
    "# -------------------------------------------------------------------\n",
    "if all_tidy:\n",
    "    master = pd.concat(all_tidy, ignore_index=True).drop_duplicates()\n",
    "    master[\"period\"] = pd.to_datetime(master[\"period\"], errors=\"coerce\")\n",
    "    master = master.dropna(subset=[\"period\"]).sort_values([\"measure\",\"period\"]).reset_index(drop=True)\n",
    "\n",
    "    master_path = DATA_PROCESSED / \"master_panel.csv\"\n",
    "    master.to_csv(master_path, index=False)\n",
    "    print(f\"Master panel written -> {master_path} ({len(master):,} rows)\")\n",
    "else:\n",
    "    print(\"No tidy data produced; master panel not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b814f8-03aa-442e-8607-cd35042a4503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674f61b4-0c56-4aa0-9116-62f1dc764246",
   "metadata": {},
   "source": [
    "## Augmented Master Panel (real MoD via YBGB, per‑capita, and QoQ/YoY growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f5caee-5969-4401-bddf-262bed1ee278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded master: data\\processed\\master_panel.csv\n",
      "Matched columns (None means not found):\n",
      "  • GDP real:      None\n",
      "  • GDP nominal:   None\n",
      "  • GDP deflator:  None\n",
      "  • Population:    None\n",
      "  • MoD total CP:  None\n",
      "  • MoD capex CP:  None\n",
      "  • MoD current CP:None\n",
      "  ! Deflator missing → cannot deflate nominal series.\n",
      "  ! Could not obtain real GDP (need either an existing real series OR nominal+deflator).\n",
      "  ! Missing nominal series for mod_total_real → skipped.\n",
      "  ! Missing nominal series for mod_cap_real → skipped.\n",
      "  ! Missing nominal series for mod_cur_real → skipped.\n",
      "  ! Population missing → skipped per‑capita calculations.\n",
      "  ! Growth skipped (missing series): mod_total_real\n",
      "  ! Growth skipped (missing series): mod_cap_real\n",
      "  ! Growth skipped (missing series): mod_cur_real\n",
      "\n",
      "Saved:\n",
      "  - data\\processed\\master_panel_20250902.csv\n",
      "  - data\\processed\\master_panel_20250902.parquet\n",
      "  - data\\processed\\master_panel_latest.csv\n"
     ]
    }
   ],
   "source": [
    "# Robust, cross‑platform augmentation script\n",
    "# — Builds real MoD series from the GDP implied deflator (YBGB)\n",
    "# — Adds per‑capita versions\n",
    "# — Adds QoQ and YoY growth rates for GDP (real) and MoD (real total/cap/current)\n",
    "# Works with a LONG master (period, measure, value) or a WIDE master.\n",
    "# Never crashes on missing inputs: it will compute what’s feasible and print a summary.\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------- Paths & I/O ------------------------------\n",
    "\n",
    "DATA_PROCESSED = Path(\"data/processed\")\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Search order (most to least specific). Absolute /mnt/data entries are harmless on Windows (just ignored if absent).\n",
    "CANDIDATE_INPUT_PATTERNS = [\n",
    "    \"/mnt/data/master_panel_*.csv\",\n",
    "    \"/mnt/data/master_panel.csv\",\n",
    "    \"data/processed/master_panel_*.csv\",\n",
    "    \"data/processed/master_panel.csv\",\n",
    "    \"master_panel_*.csv\",\n",
    "    \"master_panel.csv\",\n",
    "]\n",
    "\n",
    "def _latest_match(patterns: list[str]) -> Path | None:\n",
    "    \"\"\"Return the most recently modified existing file among the glob patterns (cross‑platform, supports absolute).\"\"\"\n",
    "    candidates: list[Path] = []\n",
    "    for pat in patterns:\n",
    "        for hit in glob(pat):\n",
    "            p = Path(hit)\n",
    "            if p.exists():\n",
    "                candidates.append(p)\n",
    "    if not candidates:\n",
    "        return None\n",
    "    return max(candidates, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "inp = _latest_match(CANDIDATE_INPUT_PATTERNS)\n",
    "if inp is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No master panel found. Expected one of:\\n  \"\n",
    "        + \"\\n  \".join(CANDIDATE_INPUT_PATTERNS)\n",
    "    )\n",
    "\n",
    "print(f\"Loaded master: {inp}\")\n",
    "raw = pd.read_csv(inp)\n",
    "\n",
    "# ----------------------------- Utilities --------------------------------\n",
    "\n",
    "def _norm(name: str) -> str:\n",
    "    \"\"\"Normalize a column/label: lowercase and keep only a-z0-9.\"\"\"\n",
    "    return \"\".join(ch for ch in str(name).lower() if ch.isalnum())\n",
    "\n",
    "def _build_norm_map(cols) -> dict[str, str]:\n",
    "    \"\"\"Map normalized -> original column names.\"\"\"\n",
    "    m = {}\n",
    "    for c in cols:\n",
    "        nc = _norm(c)\n",
    "        if nc and nc not in m:\n",
    "            m[nc] = c\n",
    "    return m\n",
    "\n",
    "def _find_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    \"\"\"Find first matching column by normalized name across synonyms.\"\"\"\n",
    "    nm = _build_norm_map(df.columns)\n",
    "    for cand in candidates:\n",
    "        nc = _norm(cand)\n",
    "        if nc in nm:\n",
    "            return nm[nc]\n",
    "    return None\n",
    "\n",
    "def _ensure_quarter_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure index is PeriodIndex(Q) named 'quarter'.\"\"\"\n",
    "    if isinstance(df.index, pd.PeriodIndex) and df.index.freqstr and df.index.freqstr.upper().startswith(\"Q\"):\n",
    "        out = df.sort_index().copy()\n",
    "        out.index.name = \"quarter\"\n",
    "        return out\n",
    "\n",
    "    # try typical date/period columns\n",
    "    for pcol in [\"quarter\",\"period\",\"date\",\"time\",\"obs_date\",\"year_quarter\",\"yearq\",\"qtr\"]:\n",
    "        if pcol in df.columns:\n",
    "            series = df[pcol].astype(str)\n",
    "            # try parse 'YYYYQn' first, else datetime→to_period('Q')\n",
    "            try:\n",
    "                idx = pd.PeriodIndex(series, freq=\"Q\")\n",
    "            except Exception:\n",
    "                idx = pd.to_datetime(series, errors=\"coerce\").dt.to_period(\"Q\")\n",
    "            out = df.set_index(idx).sort_index()\n",
    "            out.index.name = \"quarter\"\n",
    "            return out\n",
    "\n",
    "    raise ValueError(\"No quarterly period column found (expected one of: quarter/period/date/time/obs_date/year_quarter/yearq/qtr).\")\n",
    "\n",
    "def _pivot_if_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"If LONG (has measure & value), pivot to WIDE. Otherwise, just ensure Q index.\"\"\"\n",
    "    lower = {c.lower() for c in df.columns}\n",
    "    if {\"measure\",\"value\"}.issubset(lower):\n",
    "        # standardize case\n",
    "        tmp = df.rename(columns={c: c.lower() for c in df.columns})\n",
    "        tmp = _ensure_quarter_index(tmp)\n",
    "        wide = tmp.pivot_table(index=tmp.index.name, columns=\"measure\", values=\"value\", aggfunc=\"first\")\n",
    "        wide.columns.name = None\n",
    "        return wide\n",
    "    return _ensure_quarter_index(df)\n",
    "\n",
    "def _coerce_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s.astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "\n",
    "def _deflator_factor(deflator: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert deflator index (e.g., 2019=100) → factor; pass through if already ~1.x.\"\"\"\n",
    "    d = _coerce_numeric(deflator)\n",
    "    med = d.median(skipna=True)\n",
    "    if pd.isna(med):\n",
    "        raise ValueError(\"Deflator has no numeric values.\")\n",
    "    return d/100.0 if med > 10 else d\n",
    "\n",
    "def _detect_population_scale(pop: pd.Series) -> int:\n",
    "    \"\"\"If median < 1e6 assume thousands → ×1000; else persons.\"\"\"\n",
    "    med = pop.median(skipna=True)\n",
    "    if pd.isna(med):\n",
    "        return 1\n",
    "    return 1000 if med < 1_000_000 else 1\n",
    "\n",
    "def _pct_change(s: pd.Series, periods: int) -> pd.Series:\n",
    "    out = _coerce_numeric(s).div(_coerce_numeric(s.shift(periods))).sub(1)\n",
    "    return out.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def _safe_add(df: pd.DataFrame, name: str, series: pd.Series) -> None:\n",
    "    df[name] = series.reindex(df.index)\n",
    "\n",
    "# ----------------------- Normalize (long→wide) -------------------------\n",
    "\n",
    "df = _pivot_if_long(raw)\n",
    "\n",
    "# ------------------- Candidate synonyms for columns --------------------\n",
    "\n",
    "CANDS = {\n",
    "    \"gdp_real\": [\n",
    "        \"gdp_real_abmi\",\"abmi\",\"gdp_cvm_abmi\",\"gdp_cvm\",\"gdp_chain_volume\",\"gdp_volume\",\n",
    "        \"gdp_real\",\"real_gdp\",\"rgdp\",\"gdpr\"\n",
    "    ],\n",
    "    \"gdp_nominal\": [\n",
    "        \"gdp_nominal_ybha\",\"ybha\",\"gdp_current_prices\",\"gdp_nominal\",\"ngdp\",\"gdp_value\",\"gdp_cp\",\"gdp_curr\"\n",
    "    ],\n",
    "    \"deflator\": [\n",
    "        \"gdp_deflator_ybgb\",\"ybgb\",\"gdp_deflator\",\"gdp_deflator_index\",\"gdp_implied_deflator\",\n",
    "        \"implied_deflator_ybgb\",\"ybgb_q\",\"gdp_deflator_index_2019_100\"\n",
    "    ],\n",
    "    \"population\": [\n",
    "        \"population_q\",\"population\",\"uk_population_q\",\"pop_q\",\"pop\",\"uk_pop\",\"population_thousands\",\"population_millions\"\n",
    "    ],\n",
    "    \"mod_total_nom\": [\n",
    "        \"mod_total_nom\",\"mod_total_nominal\",\"mod_total_current_prices\",\"mod_total\",\"defence_total_nom\",\"defence_total\"\n",
    "    ],\n",
    "    \"mod_cap_nom\": [\n",
    "        \"mod_cap_nom\",\"mod_capital_nominal\",\"mod_capital\",\"mod_capex_nom\",\"defence_capital_nom\",\"mod_equipment_nom\"\n",
    "    ],\n",
    "    \"mod_cur_nom\": [\n",
    "        \"mod_cur_nom\",\"mod_current_nominal\",\"mod_current\",\"mod_recurrent_nom\",\"defence_current_nom\",\"mod_resource_nom\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Locate columns by synonyms (case/format tolerant)\n",
    "gdp_real_col = _find_col(df, CANDS[\"gdp_real\"])\n",
    "gdp_nom_col  = _find_col(df, CANDS[\"gdp_nominal\"])\n",
    "defl_col     = _find_col(df, CANDS[\"deflator\"])\n",
    "pop_col      = _find_col(df, CANDS[\"population\"])\n",
    "mod_tot_col  = _find_col(df, CANDS[\"mod_total_nom\"])\n",
    "mod_cap_col  = _find_col(df, CANDS[\"mod_cap_nom\"])\n",
    "mod_cur_col  = _find_col(df, CANDS[\"mod_cur_nom\"])\n",
    "\n",
    "messages = [\n",
    "    \"Matched columns (None means not found):\",\n",
    "    f\"  • GDP real:      {gdp_real_col}\",\n",
    "    f\"  • GDP nominal:   {gdp_nom_col}\",\n",
    "    f\"  • GDP deflator:  {defl_col}\",\n",
    "    f\"  • Population:    {pop_col}\",\n",
    "    f\"  • MoD total CP:  {mod_tot_col}\",\n",
    "    f\"  • MoD capex CP:  {mod_cap_col}\",\n",
    "    f\"  • MoD current CP:{mod_cur_col}\",\n",
    "]\n",
    "\n",
    "# -------------------------- Compute series -----------------------------\n",
    "\n",
    "# Deflator factor (if available)\n",
    "price_factor = None\n",
    "if defl_col is not None:\n",
    "    price_factor = _deflator_factor(df[defl_col].ffill().bfill())\n",
    "else:\n",
    "    messages.append(\"  ! Deflator missing → cannot deflate nominal series.\")\n",
    "\n",
    "# Real GDP (prefer existing; else nominal / deflator)\n",
    "gdp_real_name = None\n",
    "if gdp_real_col is not None:\n",
    "    _safe_add(df, gdp_real_col, _coerce_numeric(df[gdp_real_col]))\n",
    "    gdp_real_name = gdp_real_col\n",
    "else:\n",
    "    if (gdp_nom_col is not None) and (price_factor is not None):\n",
    "        _safe_add(df, \"gdp_real_abmi\", _coerce_numeric(df[gdp_nom_col]) / price_factor)\n",
    "        gdp_real_name = \"gdp_real_abmi\"\n",
    "        messages.append(f\"  • Derived real GDP as `{gdp_real_name}` from {gdp_nom_col}/{defl_col}.\")\n",
    "    else:\n",
    "        messages.append(\"  ! Could not obtain real GDP (need either an existing real series OR nominal+deflator).\")\n",
    "\n",
    "# Real MoD series (deflate with YBGB)\n",
    "def _deflate_mod(nom_col: str | None, out_name: str):\n",
    "    if nom_col is None:\n",
    "        messages.append(f\"  ! Missing nominal series for {out_name} → skipped.\")\n",
    "        return\n",
    "    if price_factor is None:\n",
    "        messages.append(f\"  ! Missing deflator → skipped {out_name}.\")\n",
    "        return\n",
    "    _safe_add(df, out_name, _coerce_numeric(df[nom_col]) / price_factor)\n",
    "    messages.append(f\"  • Created {out_name} from {nom_col}/{defl_col}.\")\n",
    "\n",
    "_deflate_mod(mod_tot_col, \"mod_total_real\")\n",
    "_deflate_mod(mod_cap_col, \"mod_cap_real\")\n",
    "_deflate_mod(mod_cur_col, \"mod_cur_real\")\n",
    "\n",
    "# Per‑capita series\n",
    "if pop_col is not None:\n",
    "    persons = _coerce_numeric(df[pop_col]).ffill().bfill()\n",
    "    scale = _detect_population_scale(persons)\n",
    "    persons = persons * scale\n",
    "\n",
    "    if gdp_real_name is not None:\n",
    "        _safe_add(df, \"gdp_real_pc\", _coerce_numeric(df[gdp_real_name]) / persons)\n",
    "        messages.append(\"  • Added gdp_real_pc.\")\n",
    "\n",
    "    for base in [\"mod_total_real\",\"mod_cap_real\",\"mod_cur_real\"]:\n",
    "        if base in df.columns:\n",
    "            _safe_add(df, f\"{base}_pc\", _coerce_numeric(df[base]) / persons)\n",
    "            messages.append(f\"  • Added {base}_pc.\")\n",
    "else:\n",
    "    messages.append(\"  ! Population missing → skipped per‑capita calculations.\")\n",
    "\n",
    "# QoQ & YoY growth for GDP (real) and MoD (real)\n",
    "def _add_growth(col: str):\n",
    "    if col in df.columns:\n",
    "        df[f\"{col}_qoq_pct\"] = _pct_change(df[col], 1)\n",
    "        df[f\"{col}_yoy_pct\"] = _pct_change(df[col], 4)\n",
    "        messages.append(f\"  • Added QoQ/YoY growth for {col}.\")\n",
    "    else:\n",
    "        messages.append(f\"  ! Growth skipped (missing series): {col}\")\n",
    "\n",
    "if gdp_real_name is not None:\n",
    "    _add_growth(gdp_real_name)\n",
    "for base in [\"mod_total_real\",\"mod_cap_real\",\"mod_cur_real\"]:\n",
    "    _add_growth(base)\n",
    "\n",
    "# ------------------------------- Save ----------------------------------\n",
    "\n",
    "today = pd.Timestamp.today().date()\n",
    "out_csv = DATA_PROCESSED / f\"master_panel_{today:%Y%m%d}.csv\"\n",
    "out_par = DATA_PROCESSED / f\"master_panel_{today:%Y%m%d}.parquet\"\n",
    "latest  = DATA_PROCESSED / \"master_panel_latest.csv\"\n",
    "\n",
    "df.to_csv(out_csv, index=True)\n",
    "try:\n",
    "    df.to_parquet(out_par)  # may require pyarrow/fastparquet; okay to fail silently\n",
    "except Exception:\n",
    "    pass\n",
    "df.to_csv(latest, index=True)\n",
    "\n",
    "print(\"\\n\".join(messages))\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"  - {out_csv}\")\n",
    "if out_par.exists():\n",
    "    print(f\"  - {out_par}\")\n",
    "print(f\"  - {latest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2888d7-c9da-4673-9640-389e9b5734b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
